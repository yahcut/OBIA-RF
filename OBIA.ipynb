{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------READ ME------------#\n",
    "# Suita, Osaka, 09/11/2023\n",
    "# Rajuli Amra & Yahcut mejambek Van Bakong. \n",
    "\n",
    "# Modified every path to your local computer path\n",
    "# Before using this code, it is advisable to learn the basic of orfeo toolbox algorithm, concept in Object Based Image Classification(OBIA), machine learning and satellite image\n",
    "# OBIA: https://doi.org/10.1016/j.jag.2020.102263 ; https://doi.org/10.1016/j.isprsjprs.2017.06.001 ; https://doi.org/10.3390/rs14030646 ; https://doi.org/10.1186/s40965-017-0031-6\n",
    "# OTB: https://github.com/orfeotoolbox/OTB\n",
    "# python OTB: https://github.com/orfeotoolbox/pyotb\n",
    "\n",
    "# The Raw input dataset has been pre-processed for radiometric calibration (TOA-BOA) spatial co-registration and radiometric normalization. This step is crucial for time series analysis and change detection. \n",
    "# The Norm input was surface reflectance normalize to 0-1 and added 4 spectral indices (NDVI, NDWI, BU2 and ISU)\n",
    "# Please refer to: https://doi.org/10.3390/rs9070676 ; https://doi.org/10.1016/j.compag.2019.104893 ; https://doi.org/10.1016/j.jag.2020.102290\n",
    "# Co-registration: https://github.com/GFZ/arosics/tree/main/docs\n",
    "# Radiometric Normalization: https://github.com/latmperkmol/ts-norm ; https://github.com/swegmueller/LORACCS_Mosaic_Correction (in Input Image is Planetscope)\n",
    "# If you use this code, please cite our following paper: https://doi.org/10.1016/j.rsase.2024.101438 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title='Landsat-5 TM 30m 2004'\n",
    "dataset='Landsat-5-TM'\n",
    "#Image-Input#\n",
    "input_raw='/PROCESS/chapter_2_1/input_for_obia/target_landsat_30m.tif'\n",
    "input_norm='../../input_for_obia/combine_norm/landsat-2004-30m.tif'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OBJECT BASED-IMAGE CLASSIFICATION (OBIA)#\n",
    "import pyotb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Segmentation: parameter 'spatialr' was not recognized. Available keys are ('in', 'filter', 'filter.meanshift.spatialr', 'filter.meanshift.ranger', 'filter.meanshift.thres', 'filter.meanshift.maxiter', 'filter.meanshift.minsize', 'filter.cc.expr', 'filter.watershed.threshold', 'filter.watershed.level', 'filter.mprofiles.size', 'filter.mprofiles.start', 'filter.mprofiles.step', 'filter.mprofiles.sigma', 'mode', 'mode.vector.out', 'mode.vector.outmode', 'mode.vector.inmask', 'mode.vector.neighbor', 'mode.vector.stitch', 'mode.vector.minsize', 'mode.vector.simplify', 'mode.vector.layername', 'mode.vector.fieldname', 'mode.vector.tilesize', 'mode.vector.startlabel', 'mode.vector.ogroptions', 'mode.raster.out')\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17324\\1738486579.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                    \u001b[1;34m'ranger'\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                    \u001b[1;34m'minsize'\u001b[0m    \u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                                    \u001b[1;34m'mode.vector.out'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msegment_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                                    })\n",
      "\u001b[1;32mc:\\Users\\Coast\\anaconda3\\envs\\otb\\lib\\site-packages\\pyotb\\apps.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Coast\\anaconda3\\envs\\otb\\lib\\site-packages\\pyotb\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, appname, frozen, quiet, name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;31m# Init, execute and write (auto flush only when output param was provided)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m         \u001b[1;31m# Create Output image objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         for key in filter(\n",
      "\u001b[1;32mc:\\Users\\Coast\\anaconda3\\envs\\otb\\lib\\site-packages\\pyotb\\core.py\u001b[0m in \u001b[0;36mset_parameters\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m                 raise KeyError(\n\u001b[1;32m--> 723\u001b[1;33m                     \u001b[1;34mf\"{self.name}: parameter '{key}' was not recognized. Available keys are {self.parameters_keys}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m                 )\n\u001b[0;32m    725\u001b[0m             \u001b[1;31m# When the parameter expects a list, if needed, change the value to list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Segmentation: parameter 'spatialr' was not recognized. Available keys are ('in', 'filter', 'filter.meanshift.spatialr', 'filter.meanshift.ranger', 'filter.meanshift.thres', 'filter.meanshift.maxiter', 'filter.meanshift.minsize', 'filter.cc.expr', 'filter.watershed.threshold', 'filter.watershed.level', 'filter.mprofiles.size', 'filter.mprofiles.start', 'filter.mprofiles.step', 'filter.mprofiles.sigma', 'mode', 'mode.vector.out', 'mode.vector.outmode', 'mode.vector.inmask', 'mode.vector.neighbor', 'mode.vector.stitch', 'mode.vector.minsize', 'mode.vector.simplify', 'mode.vector.layername', 'mode.vector.fieldname', 'mode.vector.tilesize', 'mode.vector.startlabel', 'mode.vector.ogroptions', 'mode.raster.out')\""
     ]
    }
   ],
   "source": [
    "#Segmentation\n",
    "#Segmentation using MSS algorithm (https://doi.org/10.1016/j.jag.2020.102263; doi: 10.1109/34.1000236; )\n",
    "segment_result=f'../SEGMENTATION/{dataset}_segment.shp'\n",
    "segmen= pyotb.Segmentation({'in'        : input_raw,\n",
    "                            'filter'    : 'meanshift',\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zonal Statistic\n",
    "zonal_stat=f'../Zonal_stat/{dataset}_zonal_stat.gpkg'\n",
    "zonal=pyotb.ZonalStatistics({'in'                   : input_norm,\n",
    "                             'inzone.vector.in'     : segment_result,\n",
    "                             'out.vector.filename'  : zonal_stat,\n",
    "                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sampling of training and validation of polygon dataset using point label interpretation using input, google earth VHR,input spectral indices (NDVI, NDWI, BI2, ISU) \n",
      "and expert assistance is performed in QGIS. Polygon sample is extracted from zonal Statistic output combine with point sample.\n",
      "Alternatively, you can import pyQGIS and perform the training together with this code. However, sampling requires visual interpretation which is better in QGIS or any GIS application \n"
     ]
    }
   ],
   "source": [
    "#Point Sample Label\n",
    "print('For sampling of training and validation of polygon dataset using point label interpretation using input, google earth VHR,input spectral indices (NDVI, NDWI, BI2, ISU) '\n",
    "'\\nand expert assistance is performed in QGIS. Polygon sample is extracted from zonal Statistic output combine with point sample.')\n",
    "\n",
    "print('Alternatively, you can import pyQGIS and perform the training together with this code. However, sampling requires visual interpretation which is better in QGIS or any GIS application ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Validating Sample Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-10 11:44:38 (INFO) [pyOTB] TrainVectorClassifier: argument for parameter \"io.vd\" was converted to list\n",
      "2023-11-10 11:44:38 (INFO) [pyOTB] TrainVectorClassifier: argument for parameter \"valid.vd\" was converted to list\n",
      "2023-11-10 11:44:38 (INFO) TrainVectorClassifier: Reading vector file 1/1\n",
      "2023-11-10 11:44:38 (INFO) TrainVectorClassifier: Reading vector file 1/1\n",
      "2023-11-10 11:44:38 (INFO) TrainVectorClassifier: Computing model file : ../model/Landsat-5_model.txt\n",
      "Training model...: 100% [**************************************************] (3s)\n",
      "Validation...: 100% [**************************************************] (0s)\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Predicted list size : 649\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: ValidationLabeledListSample size : 649\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Training performances:\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Confusion matrix (rows = reference labels, columns = produced labels):\n",
      "    [1] [2] [3] [4] [5] [6] [7] \n",
      "[1] 109   0   0   0   0   1   0 \n",
      "[2]   0  30   0   0   0   0   0 \n",
      "[3]   0   0 129   0   0   8   0 \n",
      "[4]   0   0   0  53   0   0   0 \n",
      "[5]   0   0   0   0   5   8   0 \n",
      "[6]   0   0   0   0   0 299   0 \n",
      "[7]   0   0   0   0   0   0   7 \n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Precision of class [1] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Recall of class    [1] vs all: 0.990909\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: F-score of class   [1] vs all: 0.995434\n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Precision of class [2] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Recall of class    [2] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: F-score of class   [2] vs all: 1\n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Precision of class [3] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Recall of class    [3] vs all: 0.941606\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: F-score of class   [3] vs all: 0.969925\n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Precision of class [4] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Recall of class    [4] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: F-score of class   [4] vs all: 1\n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Precision of class [5] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Recall of class    [5] vs all: 0.384615\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: F-score of class   [5] vs all: 0.555556\n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Precision of class [6] vs all: 0.946203\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Recall of class    [6] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: F-score of class   [6] vs all: 0.972358\n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Precision of class [7] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Recall of class    [7] vs all: 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: F-score of class   [7] vs all: 1\n",
      "\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: Global performance, Kappa index: 0.962374\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: mapOfIndicesValid[0] = 1\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: mapOfIndicesValid[1] = 2\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: mapOfIndicesValid[2] = 3\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: mapOfIndicesValid[3] = 4\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: mapOfIndicesValid[4] = 5\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: mapOfIndicesValid[5] = 6\n",
      "2023-11-10 11:44:42 (INFO) TrainVectorClassifier: mapOfIndicesValid[6] = 7\n"
     ]
    }
   ],
   "source": [
    "#Train Random Forest\n",
    "# Accuracy of Random forest algorithm is defined by tree number (classifier.rf.nbtrees), tree depth (classifier.rf.max), min sample leaf (classifier.rf.min),\n",
    "# and split (classifier.rf.cat). Lower value resulted in noise and low accuracy, while higher value cause high computational time and overfitting. \n",
    "# To get optimal combination, Hyperparameter-tuning and cross-validation (CV) is recommended. CV can perform using GridSearchCV algorithm. \n",
    "# Hyperparameter-tuning: https://doi.org/10.1007/s10661-015-4489-3 ; https://doi.org/10.3389/feart.2023.1188093 ; https://doi.org/10.1080/01431161.2019.1580825 ;\n",
    "#                        http://dx.doi.org/10.1016/j.isprsjprs.2016.01.011 ; https://doi.org/10.3390/rs15133241\n",
    "# GridSearch CV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "\n",
    "##INPUT#\n",
    "Training_data='../training_data/training_sample_landsat_70.gpkg'\n",
    "Validation_data='../training_data/training_sample_landsat_30.gpkg'\n",
    "\n",
    "#OUTPUT#\n",
    "model=f'../model/{dataset}_model.txt'\n",
    "matrix=f'../model/{dataset}_matrix.txt'\n",
    "\n",
    "kwargs = {\n",
    "    'feat'                  : ['mean_0','stdev_0','mean_1','stdev_1','mean_2','stdev_2','mean_3','stdev_3', 'mean_4','stdev_4','mean_5','stdev_5','mean_6','stdev_6','mean_7','stdev_7'],\n",
    "    'cfield'                : ['LC'],\n",
    "    'classifier'            : 'rf',\n",
    "    'classifier.rf.max'     : 16,\n",
    "    'classifier.rf.min'     : 2,\n",
    "    'classifier.rf.nbtrees' : 500,\n",
    "    'classifier.rf.ra'      : 0,\n",
    "    'classifier.rf.cat'     : 5,\n",
    "    'classifier.rf.var'     : 0,\n",
    "    }\n",
    "\n",
    "\n",
    "Model_training=pyotb.TrainVectorClassifier({'io.vd'             : Training_data,\n",
    "                                            'io.out'            : model,\n",
    "                                            **kwargs,\n",
    "                                            'io.confmatout'     : matrix,\n",
    "                                            'valid.vd'          : Validation_data,\n",
    "                                            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACCURACY ASSESSMENT#\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[109   0   0   0   0   1   0]\n",
      " [  0  30   0   0   0   0   0]\n",
      " [  0   0 129   0   0   8   0]\n",
      " [  0   0   0  53   0   0   0]\n",
      " [  0   0   0   0   5   8   0]\n",
      " [  0   0   0   0   0 299   0]\n",
      " [  0   0   0   0   0   0   7]]\n"
     ]
    }
   ],
   "source": [
    "#confusion_matrix\n",
    "matrix = np.loadtxt(matrix, delimiter=\",\")\n",
    "matrix = matrix.astype(int)\n",
    "\n",
    "confusion_matrix = np.array(matrix)\n",
    "print(confusion_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Producer Accuracy: 99.1% 100.0% 94.2% 100.0% 38.5% 100.0% 100.0%\n",
      "User Accuracy: 100.0% 100.0% 100.0% 100.0% 100.0% 94.6% 100.0%\n",
      "OA: 97.4%\n",
      "--------------------------------\n",
      "Precision: 1.00 1.00 1.00 1.00 1.00 0.95 1.00\n",
      "Recall: 0.99 1.00 0.94 1.00 0.38 1.00 1.00\n",
      "F1: 1.00 1.00 0.97 1.00 0.56 0.97 1.00\n",
      "F1_macro: 0.93\n",
      "--------------------------------\n",
      "Kappa Index: 0.96\n",
      "+-----+-------------------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|   0 | Class             | A      | B      | C      | D      | E      | F      | G      |\n",
      "|-----+-------------------+--------+--------+--------+--------+--------+--------+--------|\n",
      "|   1 | Precision         | 1.00   | 1.00   | 1.00   | 1.00   | 1.00   | 0.95   | 1.00   |\n",
      "|   2 | Recall            | 0.99   | 1.00   | 0.94   | 1.00   | 0.38   | 1.00   | 1.00   |\n",
      "|   3 | F1 Score          | 1.00   | 1.00   | 0.97   | 1.00   | 0.56   | 0.97   | 1.00   |\n",
      "|   4 | User Accuracy     | 100.0% | 100.0% | 100.0% | 100.0% | 100.0% | 94.6%  | 100.0% |\n",
      "|   5 | Producer Accuracy | 99.1%  | 100.0% | 94.2%  | 100.0% | 38.5%  | 100.0% | 100.0% |\n",
      "|   6 | Overall Accuracy  |        |        |        |        |        | 97.4%  |        |\n",
      "|   7 | F1 Macro          |        |        |        |        |        | 0.93   |        |\n",
      "|   8 | Kappa Index       |        |        |        |        |        | 0.96   |        |\n",
      "+-----+-------------------+--------+--------+--------+--------+--------+--------+--------+\n"
     ]
    }
   ],
   "source": [
    "#Accuracy Calculation#\n",
    "\n",
    "TP = np.diag(confusion_matrix)\n",
    "FP = confusion_matrix.sum(axis=0) - TP\n",
    "FN = confusion_matrix.sum(axis=1) - TP\n",
    "TN = confusion_matrix.sum() - (TP + FP + FN)\n",
    "\n",
    "#Producer, User and Overall Accuracy\n",
    "\n",
    "producer_accuracy = TP / confusion_matrix.sum(axis=1)\n",
    "user_accuracy = TP / confusion_matrix.sum(axis=0)\n",
    "overall_accuracy = (TP.sum()) / (TP.sum() + FP.sum())\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"Producer Accuracy:\", \" \".join([f\"{x*100:.1f}%\" for x in producer_accuracy]))\n",
    "print(\"User Accuracy:\", \" \".join([f\"{x*100:.1f}%\" for x in user_accuracy]))\n",
    "print(f\"OA: {overall_accuracy*100:.1f}%\")\n",
    "\n",
    "#Precision, Recall, F1-Score\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "F1_score=(2 * precision * recall / (precision + recall))\n",
    "F1_macro = np.mean(2 * precision * recall / (precision + recall))\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(\"Precision:\", \" \".join([f\"{x:.2f}\" for x in precision]))\n",
    "print(\"Recall:\", \" \".join([f\"{x:.2f}\" for x in recall]))\n",
    "print(\"F1:\", \" \".join([f\"{x:.2f}\" for x in F1_score]))\n",
    "print(f\"F1_macro: {F1_macro:.2f}\")\n",
    "\n",
    "#Kappa index\n",
    "\n",
    "n = confusion_matrix.sum()\n",
    "Po = np.diag(confusion_matrix).sum() / n\n",
    "Pe = (confusion_matrix.sum(axis=0) * confusion_matrix.sum(axis=1)).sum() / (n ** 2)\n",
    "kappa = (Po - Pe) / (1 - Pe)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Kappa Index: {kappa:.2f}\")\n",
    "\n",
    "#Save Accuracy as Table\n",
    "\n",
    "table = [\n",
    "    [\"Class\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\",\"G\"],\n",
    "    [\"Precision\", *[f\"{x:.2f}\" for x in precision]],\n",
    "    [\"Recall\", *[f\"{x:.2f}\" for x in recall]],\n",
    "    [\"F1 Score\", *[f\"{x:.2f}\" for x in F1_score]],\n",
    "    [\"User Accuracy\", *[f\"{x*100:.1f}%\" for x in user_accuracy]],\n",
    "    [\"Producer Accuracy\", *[f\"{x*100:.1f}%\" for x in producer_accuracy]],\n",
    "    [\"Overall Accuracy\", \"\", \"\", \"\", \"\", \"\", f\"{overall_accuracy*100:.1f}%\"],\n",
    "    [\"F1 Macro\", \"\", \"\", \"\", \"\", \"\", f\"{F1_macro:.2f}\"],\n",
    "    [\"Kappa Index\", \"\", \"\", \"\", \"\", \"\", f\"{kappa:.2f}\"]\n",
    "]\n",
    "\n",
    "accuracy_metrics=f'../result/{dataset}_accuracy.txt'\n",
    "with open(accuracy_metrics, \"w\",newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(table)    \n",
    "    \n",
    "df = pd.read_csv(accuracy_metrics, header=None)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(axis=1, how='all')\n",
    "df = df.fillna('')\n",
    "\n",
    "\n",
    "#print(df.to_string(index=False, header=False))\n",
    "print(tabulate(df, headers='firstrow', tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification#\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/new_xml_file.xml\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Your XML data\n",
    "xml_data = '../input/landsat-2004-30m.xml'\n",
    "\n",
    "# Read the XML data from the file\n",
    "with open(xml_data, 'r') as file:\n",
    "    xml_data = file.read()\n",
    "\n",
    "root = ET.fromstring(xml_data)\n",
    "\n",
    "# Iterate through each 'Statistic' element\n",
    "for statistic in root.findall('.//Statistic'):\n",
    "    values = []\n",
    "\n",
    "    # Iterate through 'StatisticVector' elements within each 'Statistic'\n",
    "    for statistic_vector in statistic.findall('.//StatisticVector'):\n",
    "        values.append(statistic_vector.get('value'))\n",
    "\n",
    "    # Remove the existing 'StatisticVector' elements\n",
    "    for statistic_vector in statistic.findall('.//StatisticVector'):\n",
    "        statistic.remove(statistic_vector)\n",
    "\n",
    "    # Create a new 'StatisticVector' element and set its 'value' attribute\n",
    "    new_statistic_vector = ET.SubElement(statistic, 'StatisticVector')\n",
    "    new_statistic_vector.set('value', '[' + ', '.join(values) + ']')\n",
    "\n",
    "# Save the modified XML to a new file\n",
    "new_xml_file = '../input/new_xml_file.xml'\n",
    "with open(new_xml_file, 'w') as file:\n",
    "    file.write(ET.tostring(root, encoding='unicode'))\n",
    "\n",
    "print(new_xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Open the satellite image\n",
    "with rasterio.open(input_norm) as src:\n",
    "\n",
    "    # Compute the image statistics\n",
    "    stats = src.read().flatten()\n",
    "    mean = stats.mean()\n",
    "    std = stats.std()\n",
    "    min = stats.min()\n",
    "    max = stats.max()\n",
    "\n",
    "# Create a new XML file with the computed statistics\n",
    "root = ET.Element(\"FeatureStatistics\")\n",
    "mean_elem = ET.SubElement(root, \"Statistic\", name=\"mean\")\n",
    "stddev_elem = ET.SubElement(root, \"Statistic\", name=\"stddev\")\n",
    "min_elem = ET.SubElement(root, \"Statistic\", name=\"min\")\n",
    "max_elem = ET.SubElement(root, \"Statistic\", name=\"max\")\n",
    "ET.SubElement(mean_elem, \"StatisticVector\", value=str(mean))\n",
    "ET.SubElement(stddev_elem, \"StatisticVector\", value=str(std))\n",
    "ET.SubElement(min_elem, \"StatisticVector\", value=str(min))\n",
    "ET.SubElement(max_elem, \"StatisticVector\", value=str(max))\n",
    "\n",
    "# Write the new XML file\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(\"../input/file_tes.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-10 11:48:18 (INFO) VectorClassifier: Model loaded\n",
      "2023-11-10 11:48:18 (INFO) VectorClassifier: mean used: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "2023-11-10 11:48:18 (INFO) VectorClassifier: standard deviation used: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "2023-11-10 11:48:18 (INFO) VectorClassifier: Loading model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Post-processing is sometime needed to reduce misclassification between sand class and built-up class (doi:10.3390/rs11030345; https://doi.org/10.3390/atmos14020240 )\n",
    "\n",
    "zonal_stat='../Zonal_stat/Landsat_2004_30_zonal_stat_fix.gpkg'\n",
    "shapefile_output=f'../result/{dataset}_vector.shp'\n",
    "classification=pyotb.VectorClassifier({'in'     : zonal_stat,\n",
    "                                       'model'  : model,\n",
    "                                       'feat'   : ['mean_0','stdev_0','mean_1','stdev_1','mean_2','stdev_2','mean_3','stdev_3', 'mean_4','stdev_4','mean_5','stdev_5','mean_6','stdev_6','mean_7','stdev_7'],\n",
    "                                       'cfield' : 'land_type',\n",
    "                                       'out'    : shapefile_output,\n",
    "                                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'meanB0', 'meanB1', 'meanB2', 'meanB3', 'varB0', 'varB1',\n",
      "       'varB2', 'varB3', 'count', 'mean_0', 'stdev_0', 'min_0', 'max_0',\n",
      "       'mean_1', 'stdev_1', 'min_1', 'max_1', 'mean_2', 'stdev_2', 'min_2',\n",
      "       'max_2', 'mean_3', 'stdev_3', 'min_3', 'max_3', 'mean_4', 'stdev_4',\n",
      "       'min_4', 'max_4', 'mean_5', 'stdev_5', 'min_5', 'max_5', 'mean_6',\n",
      "       'stdev_6', 'min_6', 'max_6', 'mean_7', 'stdev_7', 'min_7', 'max_7',\n",
      "       'land_type', 'geometry'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#select only land cover type as new vector\n",
    "\n",
    "LC= gpd.read_file(shapefile_output)\n",
    "print(LC.columns)\n",
    "\n",
    "selected_field='land_type'\n",
    "\n",
    "dissolved = LC.dissolve(by=selected_field)\n",
    "LC_vector = f'../result/{dataset}_LC_vector.shp'\n",
    "\n",
    "dissolved.to_file(LC_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-10 12:15:00 (INFO) Rasterization: Input dataset extent is (751665, 610095) (763665, 621195)\n",
      "2023-11-10 12:15:00 (INFO) Rasterization: Input dataset projection reference system is: PROJCS[\"WGS 84 / UTM zone 46N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",93],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32646\"]]\n",
      "2023-11-10 12:15:00 (INFO): Loading metadata from official product\n",
      "2023-11-10 12:15:00 (INFO) Rasterization: Output projection reference system is: PROJCS[\"WGS 84 / UTM zone 46N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",93],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32646\"]]\n",
      "2023-11-10 12:15:00 (INFO) Rasterization: Output origin: [751668, 621192]\n",
      "2023-11-10 12:15:00 (INFO) Rasterization: Output size: [2400, 2220]\n",
      "2023-11-10 12:15:00 (INFO) Rasterization: Output spacing: [5, -5]\n",
      "2023-11-10 12:15:00 (INFO): Default RAM limit for OTB is 256 MB\n",
      "2023-11-10 12:15:00 (INFO): GDAL maximum cache size is 1627 MB\n",
      "2023-11-10 12:15:00 (INFO): OTB will use at most 8 threads\n",
      "2023-11-10 12:15:00 (INFO): Estimated memory for full processing: 60.936MB (avail.: 256 MB), optimal image partitioning: 1 blocks\n",
      "2023-11-10 12:15:00 (INFO): File ../result/Landsat-5_LC_all_class.tif will be written in 1 blocks of 2400x2220 pixels\n",
      "Writing ../result/Landsat-5_LC_all_class.tif...: 100% [**************************************************] (0s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyotb.Rasterization object, id 2008366678344>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to raster_map\n",
    "\n",
    "all_class=f'../result/{dataset}_LC_all_class.tif'\n",
    "\n",
    "pyotb.Rasterization({'in'                   : LC_vector,\n",
    "                     'out'                  : all_class,\n",
    "                     'im'                   : input_norm,\n",
    "                     'mode'                 : 'attribute',\n",
    "                     'mode.attribute.field' : 'land_type',                     \n",
    "                     })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyotb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
